{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4 - Neural POS Tagger\n",
    "\n",
    "In this exercise, you are going to build a set of deep learning models on part-of-speech (POS) tagging using Tensorflow and Keras. Tensorflow is a deep learning framwork developed by Google, and Keras is a frontend library built on top of Tensorflow (or Theano, CNTK) to provide an easier way to use standard layers and networks.\n",
    "\n",
    "To complete this exercise, you will need to build deep learning models for POS tagging in Thai using NECTEC's ORCHID corpus. You will build one model for each of the following type:\n",
    "\n",
    "- Neural POS Tagging with Word Embedding using Fixed / non-Fixed Pretrained weights\n",
    "- Neural POS Tagging with Viterbi / Marginal CRF\n",
    "\n",
    "Pretrained word embeddding are already given for you to use (albeit, a very bad one). Optionally, you can use your best pretrained word embeddding from previous exercise.\n",
    "\n",
    "We also provide the code for data cleaning, preprocessing and some starter code for keras in this notebook but feel free to modify those parts to suit your needs. You can also complete this exercise using only Tensorflow (without using Keras). Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
    "\n",
    "### Don't forget to shut down your instance on Gcloud when you are not using it ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use POS data from [ORCHID corpus](https://www.nectec.or.th/corpus/index.php?league=pm), which is a POS corpus for Thai language.\n",
    "A method used to read the corpus into a list of sentences with (word, POS) pairs have been implemented already. The example usage has shown below.\n",
    "We also create a word vector for unknown word by random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data.orchid_corpus import get_sentences\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import keras.preprocessing\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('การ', 'FIXN'), ('ประชุม', 'VACT'), ('ทาง', 'NCMN'), ('วิชาการ', 'NCMN'), ('<space>', 'PUNC'), ('ครั้ง', 'CFQC'), ('ที่ 1', 'DONM')]\n"
     ]
    }
   ],
   "source": [
    "unk_emb =np.random.randn(32)\n",
    "train_data = get_sentences('train')\n",
    "test_data = get_sentences('test')\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load pretrained weight embedding using pickle. The pretrained weight is a dictionary which map a word to its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = open('basic_ff_embedding.pt', 'rb')\n",
    "embeddings = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given code below generates an indexed dataset(each word is represented by a number) for training and testing data. The index 0 is reserved for padding to help with variable length sequence. (Additionally, You can read more about padding here [https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_idx ={}\n",
    "idx_to_word ={}\n",
    "label_to_idx = {}\n",
    "for sentence in train_data:\n",
    "    for word,pos in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)+1\n",
    "            idx_to_word[word_to_idx[word]] = word\n",
    "        if pos not in label_to_idx:\n",
    "            label_to_idx[pos] = len(label_to_idx)+1\n",
    "word_to_idx['UNK'] = len(word_to_idx)\n",
    "\n",
    "n_classes = len(label_to_idx.keys())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is tweaked a little from the demo, word2features will return word index instead of features, and sent2labels will return a sequence of word indices in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i, emb):\n",
    "    word = sent[i][0]\n",
    "    if word in word_to_idx :\n",
    "        return word_to_idx[word]\n",
    "    else :\n",
    "        return word_to_idx['UNK']\n",
    "\n",
    "def sent2features(sent, emb_dict):\n",
    "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return numpy.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word for (word, label) in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29, 327,   5, 328])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_data[100], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create train and test dataset, then we use keras to post-pad the sequence to max sequence with 0. Our labels are changed to a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 366 ms, sys: 0 ns, total: 366 ms\n",
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n",
    "y_train = [sent2labels(sent) for sent in train_data]\n",
    "x_test = [sent2features(sent, embeddings) for sent in test_data]\n",
    "y_test = [sent2labels(sent) for sent in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_train=keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "x_test=keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_temp =[]\n",
    "for i in range(len(y_train)):\n",
    "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n",
    "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n",
    "del(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29 327   5 328   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0] (18500, 102)\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] (18500, 102, 48)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[100],x_train.shape)\n",
    "print(y_train[100][3],y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output from keras is a distribution of problabilities on all possible label. outputToLabel will return an indices of maximum problability from output sequence.\n",
    "\n",
    "evaluation_report is the same as in the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputToLabel(yt,seq_len):\n",
    "    out = []\n",
    "    for i in range(0,len(yt)):\n",
    "        if(i==seq_len):\n",
    "            break\n",
    "        out.append(np.argmax(yt[i]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def evaluation_report(y_true, y_pred):\n",
    "    # retrieve all tags in y_true\n",
    "    tag_set = set()\n",
    "    for sent in y_true:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    for sent in y_pred:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    tag_list = sorted(list(tag_set))\n",
    "    \n",
    "    # count correct points\n",
    "    tag_info = dict()\n",
    "    for tag in tag_list:\n",
    "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
    "\n",
    "    all_correct = 0\n",
    "    all_count = sum([len(sent) for sent in y_true])\n",
    "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
    "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
    "            if tag_true == tag_pred:\n",
    "                tag_info[tag_true]['correct_tagged'] += 1\n",
    "                all_correct += 1\n",
    "            tag_info[tag_true]['y_true'] += 1\n",
    "            tag_info[tag_pred]['y_pred'] += 1\n",
    "    accuracy = (all_correct / all_count) * 100\n",
    "            \n",
    "    # summarize and make evaluation result\n",
    "    eval_list = list()\n",
    "    for tag in tag_list:\n",
    "        eval_result = dict()\n",
    "        eval_result['tag'] = tag\n",
    "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
    "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
    "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
    "        eval_result['precision'] = precision\n",
    "        eval_result['recall'] = recall\n",
    "        eval_result['f_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
    "        \n",
    "        eval_list.append(eval_result)\n",
    "\n",
    "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f_score': ''})\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(eval_list)\n",
    "    df = df[['tag', 'precision', 'recall', 'f_score', 'correct_count']]\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is this section is separated to two groups\n",
    "\n",
    "- Neural POS Tagger (4.1)\n",
    "- Neural CRF POS Tagger (4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Neural POS Tagger  (Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple Neural POS Tagger as an example for you. This model dosen't use any pretrained word embbeding so it need to use Embedding layer to train the word embedding from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 496,208\n",
      "Trainable params: 496,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8911</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7418</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.7954</td>\n",
       "      <td>94.0713</td>\n",
       "      <td>94.4319</td>\n",
       "      <td>7759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90.5409</td>\n",
       "      <td>96.6309</td>\n",
       "      <td>93.4869</td>\n",
       "      <td>16320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9922</td>\n",
       "      <td>99.6208</td>\n",
       "      <td>99.8062</td>\n",
       "      <td>12873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.2381</td>\n",
       "      <td>89.5522</td>\n",
       "      <td>92.3077</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.5662</td>\n",
       "      <td>87.931</td>\n",
       "      <td>93.3876</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.1743</td>\n",
       "      <td>97.595</td>\n",
       "      <td>97.3842</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>67.7215</td>\n",
       "      <td>51.5663</td>\n",
       "      <td>58.5499</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>63.6111</td>\n",
       "      <td>62.2283</td>\n",
       "      <td>62.9121</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>62.9496</td>\n",
       "      <td>41.7163</td>\n",
       "      <td>50.1792</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>88.5417</td>\n",
       "      <td>98.8372</td>\n",
       "      <td>93.4066</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>97.4042</td>\n",
       "      <td>98.377</td>\n",
       "      <td>97.8882</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>89.5074</td>\n",
       "      <td>85.448</td>\n",
       "      <td>87.4306</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.5258</td>\n",
       "      <td>94.0608</td>\n",
       "      <td>94.2928</td>\n",
       "      <td>5163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>80.7415</td>\n",
       "      <td>69.8752</td>\n",
       "      <td>74.9164</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>87.6752</td>\n",
       "      <td>88.5512</td>\n",
       "      <td>88.111</td>\n",
       "      <td>2127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.5425</td>\n",
       "      <td>88.3562</td>\n",
       "      <td>92.7224</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>97.3662</td>\n",
       "      <td>99.2208</td>\n",
       "      <td>98.2847</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.6608</td>\n",
       "      <td>96.8116</td>\n",
       "      <td>97.2344</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.6207</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.7778</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>95.5234</td>\n",
       "      <td>91.5512</td>\n",
       "      <td>93.4951</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>83.399</td>\n",
       "      <td>79.2888</td>\n",
       "      <td>81.292</td>\n",
       "      <td>1271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>88.8961</td>\n",
       "      <td>97.0234</td>\n",
       "      <td>92.7821</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>90.8642</td>\n",
       "      <td>80.4372</td>\n",
       "      <td>85.3333</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>92.6421</td>\n",
       "      <td>67.0702</td>\n",
       "      <td>77.809</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>87.234</td>\n",
       "      <td>93.1818</td>\n",
       "      <td>90.1099</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>94.4444</td>\n",
       "      <td>77.8626</td>\n",
       "      <td>85.3556</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>92.2619</td>\n",
       "      <td>98.1013</td>\n",
       "      <td>95.092</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>73.5294</td>\n",
       "      <td>72.4638</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>57.6159</td>\n",
       "      <td>84.466</td>\n",
       "      <td>68.5039</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>88.6792</td>\n",
       "      <td>52.809</td>\n",
       "      <td>66.1972</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>75</td>\n",
       "      <td>48.5294</td>\n",
       "      <td>58.9286</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>82.56</td>\n",
       "      <td>91.8149</td>\n",
       "      <td>86.9419</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>62.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>93.5185</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>96.1905</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>60.5263</td>\n",
       "      <td>58.9744</td>\n",
       "      <td>59.7403</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>81.746</td>\n",
       "      <td>73.5714</td>\n",
       "      <td>77.4436</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>76.1905</td>\n",
       "      <td>80</td>\n",
       "      <td>78.0488</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>82.3529</td>\n",
       "      <td>90.3226</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=93.15</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8911  99.5929  99.7418          3670\n",
       "1                2   94.7954  94.0713  94.4319          7759\n",
       "2                3   90.5409  96.6309  93.4869         16320\n",
       "3                4   99.9922  99.6208  99.8062         12873\n",
       "4                5   95.2381  89.5522  92.3077            60\n",
       "5                6   99.5662   87.931  93.3876           459\n",
       "6                7   97.1743   97.595  97.3842          2029\n",
       "7                8   67.7215  51.5663  58.5499           214\n",
       "8                9   63.6111  62.2283  62.9121           229\n",
       "9               10   62.9496  41.7163  50.1792           350\n",
       "10              11   88.5417  98.8372  93.4066            85\n",
       "11              12   97.4042   98.377  97.8882           788\n",
       "12              13   89.5074   85.448  87.4306          3071\n",
       "13              14   94.5258  94.0608  94.2928          5163\n",
       "14              15   80.7415  69.8752  74.9164           784\n",
       "15              16   87.6752  88.5512   88.111          2127\n",
       "16              17   97.5425  88.3562  92.7224           516\n",
       "17              18   97.3662  99.2208  98.2847          1146\n",
       "18              19   97.6608  96.8116  97.2344           334\n",
       "19              20   98.6207  96.9492  97.7778           286\n",
       "20              21   95.5234  91.5512  93.4951          1387\n",
       "21              22    83.399  79.2888   81.292          1271\n",
       "22              23   88.8961  97.0234  92.7821          1369\n",
       "23              24   90.8642  80.4372  85.3333           736\n",
       "24              25   92.6421  67.0702   77.809           277\n",
       "25              26    87.234  93.1818  90.1099           164\n",
       "26              27   94.4444  77.8626  85.3556           102\n",
       "27              29   92.2619  98.1013   95.092           310\n",
       "28              30   71.4286  73.5294  72.4638            75\n",
       "29              31   57.6159   84.466  68.5039            87\n",
       "30              32   88.6792   52.809  66.1972            94\n",
       "31              33        75  48.5294  58.9286            33\n",
       "32              34     82.56  91.8149  86.9419           516\n",
       "33              35   71.4286  55.5556     62.5             5\n",
       "34              36       100      100      100            16\n",
       "35              37   93.5185  99.0196  96.1905           101\n",
       "36              38   60.5263  58.9744  59.7403            23\n",
       "37              39    81.746  73.5714  77.4436           103\n",
       "38              40       100      100      100           280\n",
       "39              41   76.1905       80  78.0488            16\n",
       "40              42       100  82.3529  90.3226            14\n",
       "41              43         0        0        -             0\n",
       "42              45         0        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=93.15                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.3 s, sys: 5.52 s, total: 36.8 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model.save_weights('/data/my_pos_no_crf.h5')\n",
    "#model.load_weights('/data/my_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Neural POS Tagger - Fix Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 1\n",
    "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. The word embedding should be fixed across training time. To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "(You may want to read about Keras's Masking layer)\n",
    "\n",
    "Optionally, you can use your own pretrained word embedding from previous homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 Neural POS Tagger - Trainable pretrained weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 2\n",
    "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. However The word embedding is trainable (not fixed). To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "Please note that the given pretrained word embedding only have weights for the vocabuary in BEST corpus from previous homework.\n",
    "\n",
    "Optionally, you can use your own pretrained word embedding from previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 3\n",
    "Compare the result between all neural tagger models in 4.1.x and provide a convincing reason and example for the result of these models (which model perform best or worst, why?)\n",
    "\n",
    "(If you use your own weight please state so in the answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write your answer here :</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 CRF Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next two tasks are to incorporate Conditional random fields (CRF) to your model. <b>You do not need to use pretrained weight</b>.\n",
    "\n",
    "Keras already implement a CRF neural model for you. However, you need to use the official extension repository for Keras library, call keras-contrib. You should read about keras-contrib crf layer before attempt this exercise section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 4\n",
    "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>viterbi algorithm</b>. Your model must use crf for loss function and metric. CRF is quite complex compare to previous example model, so you should train it with more epoch, so it can converge.\n",
    "\n",
    "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "Do not forget to save this model weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 CRF Marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 5\n",
    "\n",
    "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>marginal problabilities</b>. You <b>must not train the model</b> from scratch but use the pretrained weight from previous CRF Viterbi model.\n",
    "\n",
    "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 6\n",
    "\n",
    "Please pick the best example that can show the different between CRF that use viterbi and CRF that use marginal problabilities. Compare the result and provide a convincing reason. (which model perform better, why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write your answer here :</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
