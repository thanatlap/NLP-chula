{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Linguistically-guided LSTM  \n",
    "\n",
    "Unlike traditional machine learning methods, where features are dependent on specialized knowledge. Deep learning can automaticallly extract features. In NLP, one of the most common deep learning architectures is LSTM. It is designed to capture long-term dependencies between words. In this tutorial, we will show you how to code LSTM model for a POS tagger system from (almost) scratch using PyTorch.  In addition, we will explore PyTorch's ability to create a computational graph dynamically to create a graph that is guided by linguistic knowledge.\n",
    "## Why PyTorch?\n",
    "* PyTorch can build a computational graph dynamically.\n",
    "![dynamic_graph](http://pytorch.org/static/img/dynamic_graph.gif)\n",
    "* PyTorch is built to be deeply integrated into Python. You can use it naturally like you would use numpy / scipy / scikit-learn etc.\n",
    "\n",
    "(Image reference:http://pytorch.org/about/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brown Corpus\n",
    "The Brown Corpus of Standard American English was the first million-word computer readable corpus created in 1961 at Brown University. We use the Universal Part-of-Speech tagset in this tutorial. Knowing which Part-Of-Speech a word belongs to tells us  about its grammatical function in the sentence and its likely neighboring words. Therefore,  POS tagging is an important process in many NLP applications (e.g. Word Sense Disambiguation, Named entity Recognition).\n",
    "\n",
    "\n",
    "\n",
    "Universal Part-of-Speech Tagset \n",
    "\n",
    "|Tag | \tMeaning |\tEnglish Examples|\n",
    "| --- | --- | --- |\n",
    "|ADJ \t|adjective \t|new, good, high, special, big, local|\n",
    "|ADP \t|adposition \t|on, of, at, with, by, into, under|\n",
    "|ADV \t|adverb \t|really, already, still, early, now|\n",
    "|CONJ \t|conjunction |and, or, but, if, while, although|\n",
    "|DET \t|determiner, article |the, a, some, most, every, no, which|\n",
    "|NOUN \t|noun \t|year, home, costs, time, Africa|\n",
    "|NUM \t|numeral \t|twenty-four, fourth, 1991, 14:24|\n",
    "|PRT \t|particle \t|at, on, out, over per, that, up, with|\n",
    "|PRON \t|pronoun \t|he, their, her, its, my, I, us|\n",
    "|VERB \t|verb \t|is, say, told, given, playing, would|\n",
    "|. \t|punctuation marks \t|. , ; !|\n",
    "|X \t|other \t|ersatz, esprit, dunno, gr8, univeristy|\n",
    "\n",
    "\n",
    "(Reference: http://www.nltk.org/book/ch05.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sents=nltk.corpus.brown.tagged_sents(tagset='universal') #load the corpus from NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split data into train set and test set\n",
    "train_size = int(len(tagged_sents) * 0.9)\n",
    "train_sents = tagged_sents[:train_size]\n",
    "test_sents = tagged_sents[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('irregularities', 'NOUN'),\n",
       " ('took', 'VERB'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a sample sentence from the training set\n",
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data: Words, POS tags\n",
    "\n",
    "\n",
    "indexing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "pos_list=[]\n",
    "\n",
    "for sent in train_sents:\n",
    "    for word in sent:\n",
    "        word_list.append(word[0])\n",
    "        pos_list.append(word[1])\n",
    "\n",
    "\n",
    "#Word to Index\n",
    "word_list.append(\"UNK\") #Special Token for unknown words\n",
    "all_words = sorted(set(word_list))\n",
    "all_pos = sorted(set(pos_list))\n",
    "del word_list, pos_list\n",
    "word_to_ix = dict((c, i) for i, c in enumerate(all_words)) #convert word to index \n",
    "pos_to_ix = dict((c, i) for i, c in enumerate(all_pos)) #convert pos to index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'ADJ': 1, 'ADP': 2, 'ADV': 3, 'CONJ': 4, 'DET': 5, 'NOUN': 6, 'NUM': 7, 'PRON': 8, 'PRT': 9, 'VERB': 10, 'X': 11}\n"
     ]
    }
   ],
   "source": [
    "print(pos_to_ix) # POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_to_word = dict((v,k) for k,v in word_to_ix.items()) #convert index to word\n",
    "ix_to_pos = dict((v,k) for k,v in pos_to_ix.items())  #convert index to word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'] \n",
      " ['DET', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', '.', 'DET', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN', '.']\n"
     ]
    }
   ],
   "source": [
    "#split each input from its target\n",
    "\n",
    "input_sent =[ [ word[0] for word in sent]for sent in train_sents ] #words only\n",
    "train_targets =[ [ word[1] for word in sent]for sent in train_sents ] #POS only\n",
    "\n",
    "input_test_sent =[ [ word[0] for word in sent]for sent in test_sents ] #words only\n",
    "test_targets =[ [ word[1] for word in sent]for sent in test_sents ] #POS only\n",
    "print(input_sent[0], \"\\n\", train_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions for converting input/target text sequences to PyTorch-compatible sequences \n",
    "\n",
    "def prepare_sequence_word(input_text):\n",
    "    \"\"\"\n",
    "    Convert an input text sequence for training phrase\n",
    "    \"\"\"\n",
    "    idxs = [word_to_ix[w] for w in input_text]\n",
    "    tensor = LongTensor(idxs)\n",
    "    idxs_feat=list()\n",
    "    for word in input_text:\n",
    "        if word[0].isupper():\n",
    "            idxs_feat.append(1)\n",
    "        else:\n",
    "            idxs_feat.append(0)\n",
    "\n",
    "    tensor_cap_feat = LongTensor(idxs_feat)\n",
    "    return Variable(tensor), tensor_cap_feat\n",
    "\n",
    "def prepare_sequence_word_test(input_text):\n",
    "    \"\"\"\n",
    "    Convert an input text sequence for testing phrase\n",
    "    \"\"\"\n",
    "    idxs = list()\n",
    "    for word in input_text:\n",
    "        if word in word_to_ix:\n",
    "            idxs.append(word_to_ix[word])\n",
    "        else:\n",
    "            idxs.append(word_to_ix[\"UNK\"]) #Use UNK tag for unknown word\n",
    "   \n",
    "    tensor = LongTensor(idxs)\n",
    "    idxs_feat = list()\n",
    "    for word in input_text:\n",
    "        if word[0].isupper():\n",
    "            idxs_feat.append(1)\n",
    "        else:\n",
    "            idxs_feat.append(0)\n",
    "\n",
    "    tensor_cap_feat = LongTensor(idxs_feat)\n",
    "    return Variable(tensor, volatile=True), tensor_cap_feat\n",
    "\n",
    "def prepare_sequence_target_pos(input_label):\n",
    "    \"\"\"\n",
    "    Convert an input target sequence for training phrase\n",
    "    \"\"\"\n",
    "    idxs = [pos_to_ix[w] for w in input_label]\n",
    "    tensor = LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 16844\n",
      "  7869\n",
      "  5699\n",
      "  8350\n",
      " 10010\n",
      " 45101\n",
      "  7822\n",
      " 19580\n",
      " 34929\n",
      " 39323\n",
      "  3196\n",
      " 43450\n",
      " 42137\n",
      " 28331\n",
      " 42267\n",
      " 18506\n",
      " 38787\n",
      " 29130\n",
      "   321\n",
      " 49753\n",
      " 19857\n",
      " 35023\n",
      " 50336\n",
      " 41203\n",
      "   400\n",
      "[torch.cuda.LongTensor of size 25 (GPU 0)]\n",
      ", \n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.cuda.LongTensor of size 25 (GPU 0)]\n",
      ")\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "print(prepare_sequence_word(input_sent[0]))\n",
    "print(input_sent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linguistically guided LSTM Neural Network Architecture for POS tagger\n",
    "\n",
    "\n",
    "\n",
    "![lstm](https://cdn-images-1.medium.com/max/800/1*Hil-MwFIs_6l4naBNbUQXg.png)\n",
    "\n",
    "##### This diagram represents a typical LSTM cell, what if we want an LSTM with control-flow????\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, word_emb_dim , hidden_dim, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embedding = nn.Embedding(input_size, word_emb_dim)\n",
    "\n",
    "        ################################################################\n",
    "        self.i2ig = nn.Linear(word_emb_dim, hidden_dim) #normal inputgate\n",
    "        self.h2ig = nn.Linear(hidden_dim, hidden_dim) #normal inputgate\n",
    "        \n",
    "        self.i2ig_cap = nn.Linear(word_emb_dim, hidden_dim) #inputgate for capitalized word\n",
    "        self.h2ig_cap = nn.Linear(hidden_dim, hidden_dim) #inputgate for capitalized word\n",
    "        #####################################################################\n",
    "        self.i2ctil = nn.Linear(word_emb_dim, hidden_dim) #c_tilde (new memory cell)\n",
    "        self.h2ctil = nn.Linear(hidden_dim, hidden_dim) #c_tilde (new memory cell)\n",
    "        \n",
    "        self.i2fg = nn.Linear(word_emb_dim, hidden_dim) #forgetgate\n",
    "        self.h2fg = nn.Linear(hidden_dim, hidden_dim) #forgetgate\n",
    "        \n",
    "        self.i2og = nn.Linear(word_emb_dim, hidden_dim) #outputgate\n",
    "        self.h2og = nn.Linear(hidden_dim, hidden_dim) #outputgate\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim, output_size) #output\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        \n",
    "    def _inputGate(self, word_embed, hidden):\n",
    "        \"\"\"\n",
    "        input gate: how much the current input matters according to the past hidden state \n",
    "        and the current input\n",
    "        \"\"\"\n",
    "        i_t = F.sigmoid(self.i2ig(word_embed)+self.h2ig(hidden))\n",
    "        return i_t\n",
    "    \n",
    "    def _inputGateCap(self, word_embed, hidden):\n",
    "        \"\"\"\n",
    "        input gate for a capitalized word\n",
    "        \"\"\"\n",
    "        i_t = F.sigmoid(self.i2ig_cap(word_embed)+self.h2ig_cap(hidden))\n",
    "        return i_t\n",
    "                        \n",
    "    def _newMemoryCell(self, word_embed, hidden):\n",
    "        \"\"\"\n",
    "        New memory cell: how much the current input will be remembered according to the past hidden state \n",
    "        and the current input \n",
    "        \"\"\"\n",
    "        c_tilda = F.tanh(self.i2ctil(word_embed)+self.h2ctil(hidden)) #new memory cell\n",
    "        return c_tilda\n",
    "    \n",
    "    def _forgetGate(self, word_embed, hidden):\n",
    "        \"\"\"\n",
    "        Forget gate: how much the past memory cell matters according to the past hidden state \n",
    "        and the current input\n",
    "        \"\"\"\n",
    "        f_t = F.sigmoid(self.i2fg(word_embed)+self.h2fg(hidden))#forget gate\n",
    "        return f_t\n",
    "    \n",
    "    def _cellState(self,c_prev,c_tilda,f_t,i_t):\n",
    "        \"\"\"\n",
    "        Final memory cell: Decide how much the past memory cell will be forgotten according to the forget gate\n",
    "        and decide how much the new memory cell matters according to the input gate. \n",
    "        Then it sums both decisions to generate the final memory cell (c_t)\n",
    "        \"\"\"\n",
    "        c_t = (f_t * c_prev) + (i_t * c_tilda) \n",
    "        return c_t\n",
    "    \n",
    "    def _outputGate(self, word_embed, hidden, c_t):\n",
    "        \n",
    "        \"\"\"\n",
    "        Output gate: calculate the final hidden state according to the current input, the past hidden state\n",
    "        , and the final memory cell\n",
    "        \"\"\"\n",
    "        o_t =   F.sigmoid(self.i2og(word_embed)+self.h2og(hidden))\n",
    "        h_t  =  o_t * F.tanh(c_t)\n",
    "        return h_t \n",
    "\n",
    "    def forward(self, input_word, hidden, c_prev,feat):\n",
    "   \n",
    "        word_embed = self.word_embedding(input_word)\n",
    "        f_t = self._forgetGate(word_embed, hidden)\n",
    "        \n",
    "        \n",
    "        if feat==0:\n",
    "            i_t = self._inputGate(word_embed, hidden)\n",
    "        elif feat==1:\n",
    "            i_t = self._inputGateCap(word_embed, hidden)\n",
    "            \n",
    "        c_tilde = self._newMemoryCell(word_embed, hidden)\n",
    "        c_t = self._cellState(c_prev, c_tilde, f_t, i_t)#final memory cell\n",
    "        h_t = self._outputGate(word_embed, hidden,c_t)#final hidden state\n",
    "        lstm_out = self.hidden2tag(h_t)\n",
    "        output = self.softmax(lstm_out)\n",
    "        return output, h_t, c_t\n",
    "\n",
    "    def initHidden(self):\n",
    "        init_zeros = torch.zeros(1, self.hidden_dim).cuda() if USE_CUDA else torch.zeros(1, self.hidden_dim)\n",
    "        return Variable(init_zeros)\n",
    "    \n",
    "    def initCellState(self):\n",
    "        init_zeros = torch.zeros(1, self.hidden_dim).cuda() if USE_CUDA else torch.zeros(1, self.hidden_dim)\n",
    "        return Variable(init_zeros)\n",
    "#Initialization    \n",
    "n_words = len(word_to_ix) #number of unique words\n",
    "n_hidden = 128 # number of hidden dimensions\n",
    "n_emb_dim = 64 # number of word vector dimension\n",
    "n_categories = len(pos_to_ix) # number of POS categories\n",
    "lstm = LSTM(n_words, n_emb_dim, n_hidden, n_categories) #initilize the model\n",
    "if USE_CUDA:\n",
    "    lstm = lstm.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() #Last layer is nn.LogSoftMax, therefore NLLLoss is suitable\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) #ADAM OPTIMIZER\n",
    "\n",
    "def train(category_tensor, sentence_tensor,feat_tensor):\n",
    "    lstm.train(True)\n",
    "    hidden = lstm.initHidden() #initialize hidden state\n",
    "    cell_state = lstm.initCellState() #initiailze cell state\n",
    "    lstm.zero_grad()# reset gradient to zero\n",
    "\n",
    "    for i in range(sentence_tensor.size()[0]): # for each word in a sentence\n",
    "        output, hidden, cell_state = lstm(sentence_tensor[i], hidden,cell_state,feat_tensor[i])#fwd\n",
    "        if i != 0:\n",
    "            all_outputs=torch.cat((all_outputs,output),0) #concat output vectors together to calculate loss in one go\n",
    "        else:\n",
    "            all_outputs=output\n",
    "\n",
    "    loss = criterion(all_outputs, category_tensor)# calculate loss\n",
    "    loss.backward()#backprop\n",
    "    optimizer.step()#update parameters\n",
    "    return all_outputs, loss.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:93: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5% (23m 16s) 0.4894 \n",
      "2 10% (46m 29s) 0.2441 \n",
      "3 15% (69m 43s) 0.1441 \n",
      "4 20% (92m 56s) 0.0420 \n",
      "5 25% (116m 8s) 0.0231 \n",
      "6 30% (139m 20s) 0.0393 \n",
      "7 35% (162m 32s) 0.0332 \n",
      "8 40% (185m 45s) 0.0759 \n",
      "9 45% (208m 58s) 0.0330 \n",
      "10 50% (232m 10s) 0.0934 \n",
      "11 55% (255m 24s) 0.0481 \n",
      "12 60% (278m 40s) 0.0194 \n",
      "13 65% (301m 57s) 0.0499 \n",
      "14 70% (325m 15s) 0.0640 \n",
      "15 75% (348m 32s) 0.0402 \n",
      "16 80% (371m 49s) 0.0822 \n",
      "17 85% (395m 10s) 0.0658 \n",
      "18 90% (418m 38s) 0.0286 \n",
      "19 95% (442m 54s) 0.0345 \n",
      "20 100% (466m 28s) 0.0118 \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 20\n",
    "print_every = 1\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):#Epoch\n",
    "    for sentence, pos_tags in zip(input_sent,train_targets): #Sentence\n",
    "        sentence_tensor, feat_tensor = prepare_sequence_word(sentence) #preprocess input\n",
    "        category_tensor = prepare_sequence_target_pos(pos_tags) #preprocess tags\n",
    "        output, loss = train(category_tensor, sentence_tensor, feat_tensor) #fwd\n",
    "\n",
    "    # Print iter number,time, loss\n",
    "    if iter % print_every == 0:\n",
    "        \n",
    "        print('%d %d%% (%s) %.4f ' % (iter, iter / n_iters * 100, timeSince(start), loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "#torch.save(lstm.state_dict(), \"mylstm.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "# lstm = LSTM(n_words, n_emb_dim, n_hidden, n_categories) #initilize the model\n",
    "# if USE_CUDA:\n",
    "#     lstm = lstm.cuda()\n",
    "# lstm.load_state_dict(torch.load(\"mylstm.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(input_sent):\n",
    "    y_pred=[]\n",
    "    lstm.train(False)\n",
    "    hidden = lstm.initHidden()\n",
    "    cell_state = lstm.initCellState()\n",
    "    sentence_tensor, feat_tensor = prepare_sequence_word_test(input_sent)\n",
    "    for i in range(sentence_tensor.size()[0]):\n",
    "        output, hidden, cell_state = lstm(sentence_tensor[i], hidden,cell_state,feat_tensor[i] )\n",
    "        output=output[0].data.tolist()\n",
    "        out_ix=output.index(max(output))\n",
    "        y_pred.append(ix_to_pos[out_ix])\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict(input_test_sent[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:93: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "#predict POS tags for all sentences in the testset\n",
    "y_pred = []\n",
    "\n",
    "for test_sent in input_test_sent:\n",
    "    temp_pred = predict(test_sent)\n",
    "    y_pred.append(temp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_classification_report(y_true, y_pred):\n",
    " \n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "    tagset = sorted(set(lb.classes_)) \n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "        digits=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .     0.9999    0.9999    0.9999     15505\n",
      "        ADJ     0.9004    0.8314    0.8645      5492\n",
      "        ADP     0.9215    0.9270    0.9243      9630\n",
      "        ADV     0.9014    0.8839    0.8926      5357\n",
      "       CONJ     0.9952    0.9952    0.9952      3326\n",
      "        DET     0.9668    0.9755    0.9711     10113\n",
      "       NOUN     0.9395    0.9549    0.9472     17692\n",
      "        NUM     0.8394    0.9548    0.8934       487\n",
      "       PRON     0.9694    0.9740    0.9717      7353\n",
      "        PRT     0.8595    0.8201    0.8393      3446\n",
      "       VERB     0.9574    0.9712    0.9642     16917\n",
      "          X     0.3750    0.0726    0.1216       124\n",
      "\n",
      "avg / total     0.9493    0.9502    0.9495     95442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pos_classification_report(test_targets,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
