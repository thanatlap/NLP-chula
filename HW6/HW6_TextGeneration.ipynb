{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6 - Text Generation\n",
    "\n",
    "Barack Obama is a politician who served as the 44th President of the United States from 2009 to 2017. During that time, his speech writers have produced over 4.3MB or 730,895 words of text, not counting interviews and debates. All of Obamaâ€™s speeches are available at (http://www.americanrhetoric.com/barackobamaspeeches.html)\n",
    "\n",
    "Fortunately, samim has already scraped Obama's speeches. They are available at https://github.com/samim23/obama-rnn/.\n",
    "\n",
    "In this homework, you will try to replicate Obama's speech using a one-to-many model on **WORD LEVEL**. you are asked to do the following tasks:\n",
    "\n",
    "    Preprocessing data for keras\n",
    "    Writing a one-to-many model on **WORD LEVEL**\n",
    "    Generating somewhat Obama-liked speech\n",
    "   \n",
    "As some of you may realize, the tutorial code may not be sufficient for this dataset due to the sheer volume of the input data. You should refer to keras tutorial in text generation for training strategy for large text. (https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO1\n",
    "\n",
    "Download and Preprocess Obama speech as you see fit. (**Tip: NLTK word tokenizer**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# You may create additional cells after this cell for your preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO2\n",
    "Describe your data and preprocessing methods here. \n",
    "\n",
    "- What is your input (X,Y) ? What is its shape?\n",
    "- How do you preprocess your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO3\n",
    "\n",
    "Create Keras model for **word-level** speech generation. Your must use 'accuracy' as one of your metric.\n",
    "\n",
    "Your best model accuracy should be at least 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Write your text generation model here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO4\n",
    "\n",
    "Write code for text generation part. Speech should be generated every 5 epochs while training, and with at least 500 words length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO5\n",
    "\n",
    "Generate speech while training. Select three best speeches that represent your models progression. For example,\n",
    "\n",
    "**Epoch 1**\n",
    "\n",
    "war the we . that \n",
    " . that and the to to the to\n",
    "...\n",
    "\n",
    "**Epoch 50**\n",
    "\n",
    "war we last if change a partnership he employees one of of , jobs now loss \n",
    "...\n",
    "\n",
    "**Epoch 100**\n",
    "\n",
    "war , and the pursuit of people who were lost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Score (1.8 points)\n",
    "\n",
    "What we have used so far is a greedy algorithm, choosing word with highest problability, or choosing word using a random problability over word distribution. However, these approaches are far from optimal. As you have learned in POS Tagging lecture, it is possible to label the best POS tags over a sequence using beam search. \n",
    "\n",
    "You may read about how beam search can be used in text generation here (https://cs.stanford.edu/~zxie/textgen.pdf) \n",
    "\n",
    "Write additional code for text generation using beam search. \n",
    "\n",
    "Compare your results between text generation using greedy approach and beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write beam search function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
