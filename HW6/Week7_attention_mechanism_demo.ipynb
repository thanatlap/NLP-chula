{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism Demo on Keras: Machine Translation Example (Many-to-Many, encoder-decoder)\n",
    "\n",
    "In this demo, we will show you how to create a machine translator using Keras. This demo is inspired by Andrew Ng's deeplearning.ai course on sequence models. (Programming Assignment: Neural Machine Translation with Attention)    In this demo, we create a machine translator to translate dates in various formats  into dates in an ISO format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "We generate a toy dataset using datetime library.  A target output only comes in one format (iso format), while there are three different date format for an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generating a toy dataset\n",
    "import datetime\n",
    "base = datetime.datetime.today()\n",
    "base = datetime.date(base.year, base.month, base.day)\n",
    "date_list = [base - datetime.timedelta(days=x) for x in range(0, 15000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-18\n"
     ]
    }
   ],
   "source": [
    "target_date_list = [date.isoformat() for date in date_list] \n",
    "print(target_date_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "random.seed(42)\n",
    "input_date_list = list()\n",
    "for date in date_list:\n",
    "    random_num = randint(0, 2)\n",
    "    if random_num == 0:\n",
    "        input_date_list.append(date.strftime(\"%d/%m/%y\"))#\"11/03/02\"\n",
    "    elif random_num == 1:\n",
    "        input_date_list.append(date.strftime(\"%A %d %B %Y\")) #\"Monday 11 March 2002\"\n",
    "    elif random_num == 2: \n",
    "        input_date_list.append(date.strftime(\"%d %B %Y\")) #\"11 March 2002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 March 2018 2018-03-18\n",
      "17/03/18 2018-03-17\n",
      "16/03/18 2018-03-16\n",
      "15 March 2018 2018-03-15\n",
      "Wednesday 14 March 2018 2018-03-14\n",
      "13/03/18 2018-03-13\n",
      "12/03/18 2018-03-12\n",
      "11/03/18 2018-03-11\n",
      "10 March 2018 2018-03-10\n",
      "09/03/18 2018-03-09\n"
     ]
    }
   ],
   "source": [
    "for input_sample, target_sample in zip(input_date_list[0:10],target_date_list[0:10]):\n",
    "    print(input_sample,target_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15000 lines and 41 unique characters in your input data.\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "input_chars = list(set(''.join(input_date_list)))\n",
    "output_chars = list(set(''.join(target_date_list)))\n",
    "data_size, vocab_size = len(input_date_list), len(input_chars)\n",
    "output_vocab_size = len(output_chars)\n",
    "print('There are %d lines and %d unique characters in your input data.' % (data_size, vocab_size))\n",
    "maxlen = len( max(input_date_list, key=len)) #max input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input length: 27\n"
     ]
    }
   ],
   "source": [
    "print(\"Max input length:\", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: '/', 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9', 12: 'A', 13: 'D', 14: 'F', 15: 'J', 16: 'M', 17: 'N', 18: 'O', 19: 'S', 20: 'T', 21: 'W', 22: 'a', 23: 'b', 24: 'c', 25: 'd', 26: 'e', 27: 'g', 28: 'h', 29: 'i', 30: 'l', 31: 'm', 32: 'n', 33: 'o', 34: 'p', 35: 'r', 36: 's', 37: 't', 38: 'u', 39: 'v', 40: 'y'}\n"
     ]
    }
   ],
   "source": [
    "sorted_chars= sorted(input_chars)\n",
    "sorted_output_chars= sorted(output_chars)\n",
    "#Input\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted_chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted_chars) } #reverse dictionary\n",
    "#Output\n",
    "output_char_to_ix = { ch:i for i,ch in enumerate(sorted_output_chars) }\n",
    "ix_to_output_char = { i:ch for i,ch in enumerate(sorted_output_chars) } #reverse dictionary\n",
    "\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m=15000\n",
    "Tx=maxlen\n",
    "Ty=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 27, 41) (15000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for line in input_date_list:\n",
    "    temp=[]\n",
    "    for char in line:\n",
    "        temp.append(char_to_ix[char])\n",
    "    X.append(temp)\n",
    "Y = []\n",
    "for line in target_date_list:\n",
    "    temp=[]\n",
    "    for char in line:\n",
    "        temp.append(output_char_to_ix[char])\n",
    "    Y.append(temp)    \n",
    "\n",
    "X = pad_sequences(X,maxlen=maxlen)\n",
    "Y = pad_sequences(Y,maxlen=10)\n",
    "\n",
    "X= to_categorical(X,vocab_size)\n",
    "X=X.reshape(data_size,maxlen ,vocab_size)\n",
    "\n",
    "Y= to_categorical(Y,output_vocab_size)\n",
    "Y=Y.reshape(data_size,10 ,output_vocab_size)\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "![attn_mech](https://raw.githubusercontent.com/ekapolc/nlp_course/master/HW6/attn_mech.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These are global variables (shared layers)\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "#Attention function###\n",
    "fattn_1 = Dense(10, activation = \"tanh\")\n",
    "fattn_2 = Dense(1, activation = \"relu\")\n",
    "###\n",
    "activator = Activation(\"softmax\", name='attention_scores') \n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "\n",
    "    # Repeat the decoder hidden state to concat with encoder hidden states\n",
    "    s_prev = repeator(s_prev)\n",
    "    concat = concatenator([a,s_prev])\n",
    "    # attention function\n",
    "    e = fattn_1(concat)\n",
    "    energies =fattn_2(e)\n",
    "    # calculate attention_scores (softmax)\n",
    "    attention_scores = activator(energies)\n",
    "    #calculate a context vector\n",
    "    context = dotor([attention_scores,a])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "![rnn_model](https://raw.githubusercontent.com/ekapolc/nlp_course/master/HW6/rnn_date.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_h = 32 #hidden dimensions for encoder \n",
    "n_s = 64 #hidden dimensions for decoder\n",
    "decoder_LSTM_cell = LSTM(n_s, return_state = True) #decoder_LSTM_cell\n",
    "output_layer = Dense(output_vocab_size, activation=\"softmax\") #softmax output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_h -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    vocab_size -- size of the input vocab\n",
    "    output_vocab_size -- size of the output vocab\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model\n",
    "    X = Input(shape=(Tx, vocab_size))\n",
    "    # Define hidden state and cell state for decoder_LSTM_Cell\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = list()\n",
    "\n",
    "    #Encoder Bi-LSTM\n",
    "    h = Bidirectional(LSTM(n_h, return_sequences=True),input_shape=(m, Tx, n_h*2))(X)\n",
    "  \n",
    "    #Iterate for Ty steps (Decoding)\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        #Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
    "        context = one_step_attention(h, s)\n",
    "       \n",
    "        # Feed the context vector to the decoder LSTM cell\n",
    "        s, _, c = decoder_LSTM_cell(context,initial_state=[s,c])\n",
    "           \n",
    "        # Pass the decoder hidden output to the output layer (softmax)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Append an output list with the current output\n",
    "        outputs.append(out)\n",
    "    \n",
    "    #Create model instance\n",
    "    model = Model(inputs=[X,s0,c0],outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_h, n_s, vocab_size, output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 27, 41)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "s0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 27, 64)        18944       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 27, 64)        0           s0[0][0]                         \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 27, 128)       0           bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[1][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[2][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[3][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[4][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[5][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[6][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[7][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[8][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 27, 10)        1290        concatenate_1[0][0]              \n",
      "                                                                   concatenate_1[1][0]              \n",
      "                                                                   concatenate_1[2][0]              \n",
      "                                                                   concatenate_1[3][0]              \n",
      "                                                                   concatenate_1[4][0]              \n",
      "                                                                   concatenate_1[5][0]              \n",
      "                                                                   concatenate_1[6][0]              \n",
      "                                                                   concatenate_1[7][0]              \n",
      "                                                                   concatenate_1[8][0]              \n",
      "                                                                   concatenate_1[9][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 27, 1)         11          dense_1[0][0]                    \n",
      "                                                                   dense_1[1][0]                    \n",
      "                                                                   dense_1[2][0]                    \n",
      "                                                                   dense_1[3][0]                    \n",
      "                                                                   dense_1[4][0]                    \n",
      "                                                                   dense_1[5][0]                    \n",
      "                                                                   dense_1[6][0]                    \n",
      "                                                                   dense_1[7][0]                    \n",
      "                                                                   dense_1[8][0]                    \n",
      "                                                                   dense_1[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attention_scores (Activation)    (None, 27, 1)         0           dense_2[0][0]                    \n",
      "                                                                   dense_2[1][0]                    \n",
      "                                                                   dense_2[2][0]                    \n",
      "                                                                   dense_2[3][0]                    \n",
      "                                                                   dense_2[4][0]                    \n",
      "                                                                   dense_2[5][0]                    \n",
      "                                                                   dense_2[6][0]                    \n",
      "                                                                   dense_2[7][0]                    \n",
      "                                                                   dense_2[8][0]                    \n",
      "                                                                   dense_2[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1, 64)         0           attention_scores[0][0]           \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_scores[1][0]           \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_scores[2][0]           \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_scores[3][0]           \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_scores[4][0]           \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_scores[5][0]           \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_scores[6][0]           \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_scores[7][0]           \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_scores[8][0]           \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_scores[9][0]           \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "c0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 64), (None, 6 33024       dot_1[0][0]                      \n",
      "                                                                   s0[0][0]                         \n",
      "                                                                   c0[0][0]                         \n",
      "                                                                   dot_1[1][0]                      \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "                                                                   dot_1[2][0]                      \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[1][2]                     \n",
      "                                                                   dot_1[3][0]                      \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[2][2]                     \n",
      "                                                                   dot_1[4][0]                      \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[3][2]                     \n",
      "                                                                   dot_1[5][0]                      \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[4][2]                     \n",
      "                                                                   dot_1[6][0]                      \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[5][2]                     \n",
      "                                                                   dot_1[7][0]                      \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[6][2]                     \n",
      "                                                                   dot_1[8][0]                      \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[7][2]                     \n",
      "                                                                   dot_1[9][0]                      \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[8][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 11)            715         lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[9][0]                     \n",
      "====================================================================================================\n",
      "Total params: 53,984\n",
      "Trainable params: 53,984\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr= 0.01, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Y.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 18s - loss: 12.7681 - dense_3_loss_1: 0.4985 - dense_3_loss_2: 0.5289 - dense_3_loss_3: 1.1394 - dense_3_loss_4: 2.2460 - dense_3_loss_5: 0.8241 - dense_3_loss_6: 1.0804 - dense_3_loss_7: 2.0244 - dense_3_loss_8: 0.8711 - dense_3_loss_9: 1.4234 - dense_3_loss_10: 2.1319 - dense_3_acc_1: 0.8742 - dense_3_acc_2: 0.8511 - dense_3_acc_3: 0.6051 - dense_3_acc_4: 0.2209 - dense_3_acc_5: 0.7788 - dense_3_acc_6: 0.5636 - dense_3_acc_7: 0.2467 - dense_3_acc_8: 0.7443 - dense_3_acc_9: 0.4276 - dense_3_acc_10: 0.2357    \n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 14s - loss: 4.0360 - dense_3_loss_1: 0.0636 - dense_3_loss_2: 0.0535 - dense_3_loss_3: 0.3745 - dense_3_loss_4: 0.7920 - dense_3_loss_5: 0.0188 - dense_3_loss_6: 0.2373 - dense_3_loss_7: 0.9851 - dense_3_loss_8: 0.0174 - dense_3_loss_9: 0.5399 - dense_3_loss_10: 0.9539 - dense_3_acc_1: 0.9819 - dense_3_acc_2: 0.9817 - dense_3_acc_3: 0.8570 - dense_3_acc_4: 0.7469 - dense_3_acc_5: 0.9997 - dense_3_acc_6: 0.9255 - dense_3_acc_7: 0.6277 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.7968 - dense_3_acc_10: 0.6357    \n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 14s - loss: 2.0086 - dense_3_loss_1: 0.0314 - dense_3_loss_2: 0.0277 - dense_3_loss_3: 0.2489 - dense_3_loss_4: 0.2976 - dense_3_loss_5: 0.0088 - dense_3_loss_6: 0.1214 - dense_3_loss_7: 0.4903 - dense_3_loss_8: 0.0107 - dense_3_loss_9: 0.3014 - dense_3_loss_10: 0.4703 - dense_3_acc_1: 0.9915 - dense_3_acc_2: 0.9905 - dense_3_acc_3: 0.8868 - dense_3_acc_4: 0.9309 - dense_3_acc_5: 0.9999 - dense_3_acc_6: 0.9647 - dense_3_acc_7: 0.8573 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8857 - dense_3_acc_10: 0.8177    \n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 14s - loss: 1.1957 - dense_3_loss_1: 0.0127 - dense_3_loss_2: 0.0098 - dense_3_loss_3: 0.1845 - dense_3_loss_4: 0.1722 - dense_3_loss_5: 0.0050 - dense_3_loss_6: 0.0701 - dense_3_loss_7: 0.2627 - dense_3_loss_8: 0.0082 - dense_3_loss_9: 0.1685 - dense_3_loss_10: 0.3020 - dense_3_acc_1: 0.9975 - dense_3_acc_2: 0.9969 - dense_3_acc_3: 0.9159 - dense_3_acc_4: 0.9645 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9781 - dense_3_acc_7: 0.9406 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.9490 - dense_3_acc_10: 0.8738    \n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.7455 - dense_3_loss_1: 0.0062 - dense_3_loss_2: 0.0039 - dense_3_loss_3: 0.1445 - dense_3_loss_4: 0.1179 - dense_3_loss_5: 0.0037 - dense_3_loss_6: 0.0427 - dense_3_loss_7: 0.1767 - dense_3_loss_8: 0.0072 - dense_3_loss_9: 0.0784 - dense_3_loss_10: 0.1644 - dense_3_acc_1: 0.9993 - dense_3_acc_2: 0.9994 - dense_3_acc_3: 0.9377 - dense_3_acc_4: 0.9782 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9881 - dense_3_acc_7: 0.9631 - dense_3_acc_8: 0.9999 - dense_3_acc_9: 0.9899 - dense_3_acc_10: 0.9598    \n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.4824 - dense_3_loss_1: 0.0035 - dense_3_loss_2: 0.0021 - dense_3_loss_3: 0.1080 - dense_3_loss_4: 0.0808 - dense_3_loss_5: 0.0028 - dense_3_loss_6: 0.0247 - dense_3_loss_7: 0.1258 - dense_3_loss_8: 0.0062 - dense_3_loss_9: 0.0412 - dense_3_loss_10: 0.0875 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9630 - dense_3_acc_4: 0.9888 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9951 - dense_3_acc_7: 0.9760 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.9979 - dense_3_acc_10: 0.9911    \n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 15s - loss: 0.3289 - dense_3_loss_1: 0.0025 - dense_3_loss_2: 0.0017 - dense_3_loss_3: 0.0749 - dense_3_loss_4: 0.0551 - dense_3_loss_5: 0.0024 - dense_3_loss_6: 0.0134 - dense_3_loss_7: 0.0911 - dense_3_loss_8: 0.0048 - dense_3_loss_9: 0.0272 - dense_3_loss_10: 0.0558 - dense_3_acc_1: 0.9999 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9813 - dense_3_acc_4: 0.9949 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9990 - dense_3_acc_7: 0.9853 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.9994 - dense_3_acc_10: 0.9973    \n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 15s - loss: 0.2342 - dense_3_loss_1: 0.0018 - dense_3_loss_2: 0.0013 - dense_3_loss_3: 0.0473 - dense_3_loss_4: 0.0375 - dense_3_loss_5: 0.0020 - dense_3_loss_6: 0.0090 - dense_3_loss_7: 0.0691 - dense_3_loss_8: 0.0039 - dense_3_loss_9: 0.0210 - dense_3_loss_10: 0.0412 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 0.9999 - dense_3_acc_3: 0.9935 - dense_3_acc_4: 0.9979 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9997 - dense_3_acc_7: 0.9919 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.9998 - dense_3_acc_10: 0.9984    \n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.1771 - dense_3_loss_1: 0.0015 - dense_3_loss_2: 0.0011 - dense_3_loss_3: 0.0325 - dense_3_loss_4: 0.0281 - dense_3_loss_5: 0.0017 - dense_3_loss_6: 0.0069 - dense_3_loss_7: 0.0533 - dense_3_loss_8: 0.0033 - dense_3_loss_9: 0.0166 - dense_3_loss_10: 0.0320 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9969 - dense_3_acc_4: 0.9989 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9999 - dense_3_acc_7: 0.9953 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.9999 - dense_3_acc_10: 0.9993    \n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.1376 - dense_3_loss_1: 0.0013 - dense_3_loss_2: 8.9576e-04 - dense_3_loss_3: 0.0237 - dense_3_loss_4: 0.0220 - dense_3_loss_5: 0.0014 - dense_3_loss_6: 0.0054 - dense_3_loss_7: 0.0419 - dense_3_loss_8: 0.0028 - dense_3_loss_9: 0.0135 - dense_3_loss_10: 0.0246 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9995 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9999 - dense_3_acc_7: 0.9977 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 0.9999    \n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.1128 - dense_3_loss_1: 0.0011 - dense_3_loss_2: 8.0418e-04 - dense_3_loss_3: 0.0186 - dense_3_loss_4: 0.0181 - dense_3_loss_5: 0.0012 - dense_3_loss_6: 0.0044 - dense_3_loss_7: 0.0339 - dense_3_loss_8: 0.0025 - dense_3_loss_9: 0.0114 - dense_3_loss_10: 0.0207 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9989 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 1.0000 - dense_3_acc_7: 0.9987 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 0.9999    \n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.0948 - dense_3_loss_1: 0.0010 - dense_3_loss_2: 7.0551e-04 - dense_3_loss_3: 0.0149 - dense_3_loss_4: 0.0156 - dense_3_loss_5: 0.0011 - dense_3_loss_6: 0.0038 - dense_3_loss_7: 0.0284 - dense_3_loss_8: 0.0022 - dense_3_loss_9: 0.0099 - dense_3_loss_10: 0.0172 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9993 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 1.0000 - dense_3_acc_7: 0.9995 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 0.9999        \n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.0814 - dense_3_loss_1: 9.0839e-04 - dense_3_loss_2: 6.2599e-04 - dense_3_loss_3: 0.0125 - dense_3_loss_4: 0.0135 - dense_3_loss_5: 9.9856e-04 - dense_3_loss_6: 0.0033 - dense_3_loss_7: 0.0240 - dense_3_loss_8: 0.0020 - dense_3_loss_9: 0.0086 - dense_3_loss_10: 0.0150 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9996 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 1.0000 - dense_3_acc_7: 0.9997 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 0.9999\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.0704 - dense_3_loss_1: 8.3743e-04 - dense_3_loss_2: 5.6858e-04 - dense_3_loss_3: 0.0104 - dense_3_loss_4: 0.0118 - dense_3_loss_5: 9.0732e-04 - dense_3_loss_6: 0.0029 - dense_3_loss_7: 0.0206 - dense_3_loss_8: 0.0018 - dense_3_loss_9: 0.0076 - dense_3_loss_10: 0.0129 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9996 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 1.0000 - dense_3_acc_7: 0.9997 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 0.9999    \n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 16s - loss: 0.0622 - dense_3_loss_1: 7.8937e-04 - dense_3_loss_2: 5.1794e-04 - dense_3_loss_3: 0.0090 - dense_3_loss_4: 0.0105 - dense_3_loss_5: 8.2599e-04 - dense_3_loss_6: 0.0026 - dense_3_loss_7: 0.0180 - dense_3_loss_8: 0.0016 - dense_3_loss_9: 0.0068 - dense_3_loss_10: 0.0116 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9997 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 1.0000 - dense_3_acc_7: 0.9999 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 1.0000    \n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.0551 - dense_3_loss_1: 7.0903e-04 - dense_3_loss_2: 4.7157e-04 - dense_3_loss_3: 0.0076 - dense_3_loss_4: 0.0094 - dense_3_loss_5: 7.7848e-04 - dense_3_loss_6: 0.0023 - dense_3_loss_7: 0.0159 - dense_3_loss_8: 0.0015 - dense_3_loss_9: 0.0061 - dense_3_loss_10: 0.0104 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9997 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 1.0000 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 1.0000    \n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.0497 - dense_3_loss_1: 6.6209e-04 - dense_3_loss_2: 4.3113e-04 - dense_3_loss_3: 0.0066 - dense_3_loss_4: 0.0086 - dense_3_loss_5: 7.1293e-04 - dense_3_loss_6: 0.0021 - dense_3_loss_7: 0.0143 - dense_3_loss_8: 0.0014 - dense_3_loss_9: 0.0056 - dense_3_loss_10: 0.0093 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9999 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 1.0000 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 1.0000    \n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.0448 - dense_3_loss_1: 6.1533e-04 - dense_3_loss_2: 4.0776e-04 - dense_3_loss_3: 0.0059 - dense_3_loss_4: 0.0078 - dense_3_loss_5: 6.6994e-04 - dense_3_loss_6: 0.0019 - dense_3_loss_7: 0.0127 - dense_3_loss_8: 0.0013 - dense_3_loss_9: 0.0050 - dense_3_loss_10: 0.0085 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9999 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 1.0000 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 1.0000    \n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.0411 - dense_3_loss_1: 5.7656e-04 - dense_3_loss_2: 3.7351e-04 - dense_3_loss_3: 0.0053 - dense_3_loss_4: 0.0072 - dense_3_loss_5: 6.2185e-04 - dense_3_loss_6: 0.0018 - dense_3_loss_7: 0.0116 - dense_3_loss_8: 0.0012 - dense_3_loss_9: 0.0047 - dense_3_loss_10: 0.0078 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 0.9999 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 1.0000 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 1.0000    \n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 14s - loss: 0.0376 - dense_3_loss_1: 5.3290e-04 - dense_3_loss_2: 3.4339e-04 - dense_3_loss_3: 0.0048 - dense_3_loss_4: 0.0066 - dense_3_loss_5: 5.8107e-04 - dense_3_loss_6: 0.0016 - dense_3_loss_7: 0.0106 - dense_3_loss_8: 0.0011 - dense_3_loss_9: 0.0044 - dense_3_loss_10: 0.0071 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 1.0000 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 1.0000 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 1.0000 - dense_3_acc_10: 1.0000    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1770d4588>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, s0, c0], outputs, epochs=20, batch_size=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do some \"translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999-05-33\n",
      "2009-10-15\n",
      "2016-08-30\n",
      "2000-07-11\n",
      "2018-05-19\n",
      "2001-03-33\n",
      "2001-03-10\n"
     ]
    }
   ],
   "source": [
    "def prep_input(input_list):\n",
    "    X = []\n",
    "    for line in input_list:\n",
    "        temp=[]\n",
    "        for char in line:\n",
    "            temp.append(char_to_ix[char])\n",
    "        X.append(temp)\n",
    "    X = pad_sequences(X,maxlen=maxlen)\n",
    "    X= to_categorical(X,vocab_size)\n",
    "    X=X.reshape(len(input_list),maxlen ,vocab_size)\n",
    "    \n",
    "    return X\n",
    "\n",
    "EXAMPLES = ['3 May 1999', '05 October 2009', '30 August 2016', '11 July 2000', 'Saturday 19 May 2018', '3 March 2001', '1 March 2001']\n",
    "EXAMPLES = prep_input(EXAMPLES)\n",
    "prediction = model.predict([EXAMPLES , s0, c0])\n",
    "prediction = np.swapaxes(prediction,0,1)\n",
    "prediction = np.argmax(prediction, axis = -1)\n",
    "\n",
    "for j in range(len(prediction)):\n",
    "    output = \"\".join([ix_to_output_char[int(i)] for i in prediction[j]])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Plot the attention map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
